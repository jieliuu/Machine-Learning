# Bagging and Random Forest
## Bagging
&emsp; Bagging 属于并行式集成学习的方法。Bagginng 是基于自助采样法。给定每个包含m个样本的数据集，随机选取一个样本放入采样集，
在把该样本放回到原始的数据集中，经过m次这样的操作后,得到含m个样本的采样集。初始数据集中的样本可能多次出现在采样集中，
可能一次也没有出现，初始数据集中大约有66.7%的样本出现在采样集中。按照上述的操作可以得到T个包含m个样本的采样集。
利用这T个采样集训练得到T个基学习器，再将这些基学习器进行集成。在对预测进行输出结合时，对于分类任务，使用简单投票法，
对于回归任务，使用简单平均法。<br>
&emsp; 从偏差-方差分解的角度上来看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。
## Random Forest
&emsp; 随机森林是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。
给定大小为d的属性集，从这个属性集中随机选取包含k个属性的子集，然后再从这个子集中选择最优的属性进行划分。k控制了随机性的引入程度，一般情况下k = log(d)
随机森林的生成方法：
1. 通过自助采样法得到包含m个样本的采样集
2. 从包含d个属性的属性集中随机选取包含k个属性的属性子集
3. 通过采样集和属性子集训练得到一棵决策树
4. 重复上述步骤生成随机森林。

&emsp; 在进行预测时，使用简单投票法。随机森林基学习器的多样性不仅来自样本扰动，还来自属性扰动。随着基学习器数目的增加，随机森林通常会收敛到更低的泛化误差
。因为在个体决策树构建的过程中，随机森林只需用到一个属性子集，所以随机森林的训练效率要高于Bagging

