<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# Bagging and Random Forest
## Bagging
&emsp; Bagging 属于并行式集成学习的方法。Bagginng 是基于自助采样法。给定每个包含m个样本的数据集，随机选取一个样本放入采样集，
在把该样本放回到原始的数据集中，经过m次这样的操作后,得到含m个样本的采样集。初始数据集中的样本可能多次出现在采样集中，
可能一次也没有出现，初始数据集中大约有66.7%的样本出现在采样集中。按照上述的操作可以得到T个包含m个样本的采样集。
利用这T个采样集训练得到T个基学习器，再将这些基学习器进行集成。在对预测进行输出结合时，对于分类任务，使用简单投票法，
对于回归任务，使用简单平均法。<br>
&emsp; 从偏差-方差分解的角度上来看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。
## 随机森林
&emsp; 随机森林是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。
给定大小为d的属性集，从这个属性集中随机选取包含k个属性的子集，然后再从这个子集中选择最优的属性进行划分。k控制了随机性的引入程度，一般情况下k = log(d)
随机森林的生成方法：
1. 通过自助采样法得到包含m个样本的采样集
2. 从包含d个属性的属性集中随机选取包含k个属性的属性子集
3. 通过采样集和属性子集训练得到一棵决策树
4. 重复上述步骤生成随机森林。

&emsp; 在进行预测时，使用简单投票法。随机森林基学习器的多样性不仅来自样本扰动，还来自属性扰动。随着基学习器数目的增加，随机森林通常会收敛到更低的泛化误差。因为在个体决策树构建的过程中，随机森林只需用到一个属性子集，所以随机森林的训练效率要高于Bagging

### 随机森林缺失值处理
1. （na.roughfix）对于训练集，同一个Class下的数据，如果是分类变量缺失，用众数补上， 如果是连续型变量缺失，用中位数补。
2. （rflmpute）这个方法计算量大，与第一种方法比不好判断孰优孰劣。先用第一种方法补上缺失值，然后构建森林并计算proximity matrix。proximity matrix是随机森林的一个特点。样本之间的相似度通过一个N* N的相似矩阵刻画， 矩阵元素p(i, j)代表样本i和j之间的相似度， 具体数值为样本i和j在随机森林生成的所有树中，出现同一叶节点的个数，最后归一化（除以总共生成树的数目）。如果是分类变量， 则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。

### 随机森林评估特征重要性
GI_m=\sum_{k=1}^{|K|}\sum_{k' \neq k } p_{mk} p_{mk'}=1-\sum_{k=1}^{|K|}p_{mk}^2
